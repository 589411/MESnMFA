"""
CCL æ¨¹è„‚é…æ–¹é æ¸¬æ¨¡å‹
====================
ä½¿ç”¨ Random Forest å’Œ XGBoost é€²è¡Œå¤šç›®æ¨™é æ¸¬
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
import json
import pickle
from pathlib import Path

# æ©Ÿå™¨å­¸ç¿’
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# å˜—è©¦å°å…¥ XGBoost (å¯é¸)
try:
    from xgboost import XGBRegressor
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("Warning: XGBoost not installed. Using GradientBoosting as fallback.")


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    input_cols: List[str] = None
    output_cols: List[str] = None
    test_size: float = 0.2
    random_state: int = 42
    n_estimators: int = 100
    
    def __post_init__(self):
        if self.input_cols is None:
            self.input_cols = [
                'Hardener_Eq_Ratio', 'Filler_Vol_Pct', 'FR_Wt_Pct',
                'Toughener_Wt_Pct', 'Wash_Cycles', 'Residual_Cl_ppm'
            ]
        if self.output_cols is None:
            self.output_cols = [
                'Dk_10GHz', 'Df_10GHz', 'Peel_Strength_N_mm', 'Tg_C', 'CTE_ppm'
            ]


class CCLPredictor:
    """CCL æ¨¹è„‚é…æ–¹é æ¸¬å™¨"""
    
    def __init__(self, config: Optional[ModelConfig] = None):
        self.config = config or ModelConfig()
        self.models: Dict[str, any] = {}
        self.scaler_X: Optional[StandardScaler] = None
        self.scaler_y: Dict[str, StandardScaler] = {}
        self.feature_importance: Dict[str, pd.DataFrame] = {}
        self.metrics: Dict[str, Dict[str, float]] = {}
        self._is_trained = False
        
    def load_data(self, filepath: str) -> pd.DataFrame:
        """è¼‰å…¥æ•¸æ“š"""
        self.df = pd.read_csv(filepath)
        print(f"âœ… è¼‰å…¥ {len(self.df)} ç­†æ•¸æ“š")
        return self.df
    
    def prepare_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """æº–å‚™è¨“ç·´/æ¸¬è©¦æ•¸æ“š"""
        X = self.df[self.config.input_cols].values
        y = self.df[self.config.output_cols].values
        
        # åˆ†å‰²æ•¸æ“š
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, 
            test_size=self.config.test_size,
            random_state=self.config.random_state
        )
        
        # æ¨™æº–åŒ–è¼¸å…¥
        self.scaler_X = StandardScaler()
        X_train_scaled = self.scaler_X.fit_transform(X_train)
        X_test_scaled = self.scaler_X.transform(X_test)
        
        print(f"âœ… è¨“ç·´é›†: {len(X_train)} ç­†, æ¸¬è©¦é›†: {len(X_test)} ç­†")
        
        return X_train_scaled, X_test_scaled, y_train, y_test
    
    def train(self, model_type: str = 'random_forest') -> Dict[str, float]:
        """
        è¨“ç·´æ¨¡å‹
        
        Parameters
        ----------
        model_type : str
            'random_forest' æˆ– 'xgboost'
        
        Returns
        -------
        Dict[str, float]
            å„ç›®æ¨™çš„ RÂ² åˆ†æ•¸
        """
        X_train, X_test, y_train, y_test = self.prepare_data()
        
        print(f"\nğŸš€ é–‹å§‹è¨“ç·´ {model_type.upper()} æ¨¡å‹...")
        print("=" * 50)
        
        # ç‚ºæ¯å€‹è¼¸å‡ºè®Šæ•¸è¨“ç·´ç¨ç«‹æ¨¡å‹
        for i, target in enumerate(self.config.output_cols):
            print(f"\nè¨“ç·´ {target}...")
            
            # é¸æ“‡æ¨¡å‹
            if model_type == 'random_forest':
                model = RandomForestRegressor(
                    n_estimators=self.config.n_estimators,
                    random_state=self.config.random_state,
                    n_jobs=-1
                )
            elif model_type == 'xgboost' and HAS_XGBOOST:
                model = XGBRegressor(
                    n_estimators=self.config.n_estimators,
                    random_state=self.config.random_state,
                    n_jobs=-1,
                    verbosity=0
                )
            else:
                model = GradientBoostingRegressor(
                    n_estimators=self.config.n_estimators,
                    random_state=self.config.random_state
                )
            
            # è¨“ç·´
            y_target_train = y_train[:, i]
            y_target_test = y_test[:, i]
            
            model.fit(X_train, y_target_train)
            
            # é æ¸¬
            y_pred_train = model.predict(X_train)
            y_pred_test = model.predict(X_test)
            
            # è©•ä¼°
            r2_train = r2_score(y_target_train, y_pred_train)
            r2_test = r2_score(y_target_test, y_pred_test)
            mae_test = mean_absolute_error(y_target_test, y_pred_test)
            rmse_test = np.sqrt(mean_squared_error(y_target_test, y_pred_test))
            
            self.metrics[target] = {
                'r2_train': r2_train,
                'r2_test': r2_test,
                'mae': mae_test,
                'rmse': rmse_test
            }
            
            print(f"  RÂ² (train): {r2_train:.4f}")
            print(f"  RÂ² (test):  {r2_test:.4f}")
            print(f"  MAE:        {mae_test:.4f}")
            print(f"  RMSE:       {rmse_test:.4f}")
            
            # å„²å­˜æ¨¡å‹
            self.models[target] = model
            
            # ç‰¹å¾µé‡è¦æ€§
            if hasattr(model, 'feature_importances_'):
                importance = pd.DataFrame({
                    'feature': self.config.input_cols,
                    'importance': model.feature_importances_
                }).sort_values('importance', ascending=False)
                self.feature_importance[target] = importance
        
        self._is_trained = True
        print("\n" + "=" * 50)
        print("âœ… è¨“ç·´å®Œæˆ!")
        
        return {k: v['r2_test'] for k, v in self.metrics.items()}
    
    def predict(self, formulation: Dict[str, float]) -> Dict[str, float]:
        """
        é æ¸¬å–®ä¸€é…æ–¹çš„ç‰©ç†æ€§è³ª
        
        Parameters
        ----------
        formulation : Dict[str, float]
            é…æ–¹åƒæ•¸ï¼Œä¾‹å¦‚:
            {
                'Hardener_Eq_Ratio': 0.95,
                'Filler_Vol_Pct': 35,
                'FR_Wt_Pct': 5,
                'Toughener_Wt_Pct': 3,
                'Wash_Cycles': 4,
                'Residual_Cl_ppm': 15
            }
        
        Returns
        -------
        Dict[str, float]
            é æ¸¬çš„ç‰©ç†æ€§è³ª
        """
        if not self._is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´ï¼Œè«‹å…ˆå‘¼å« train()")
        
        # æº–å‚™è¼¸å…¥
        X = np.array([[formulation.get(col, 0) for col in self.config.input_cols]])
        X_scaled = self.scaler_X.transform(X)
        
        # é æ¸¬å„ç›®æ¨™
        predictions = {}
        for target in self.config.output_cols:
            pred = self.models[target].predict(X_scaled)[0]
            predictions[target] = round(pred, 4)
        
        return predictions
    
    def predict_batch(self, formulations: pd.DataFrame) -> pd.DataFrame:
        """æ‰¹æ¬¡é æ¸¬å¤šå€‹é…æ–¹"""
        if not self._is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´ï¼Œè«‹å…ˆå‘¼å« train()")
        
        X = formulations[self.config.input_cols].values
        X_scaled = self.scaler_X.transform(X)
        
        results = formulations.copy()
        for target in self.config.output_cols:
            results[f'{target}_pred'] = self.models[target].predict(X_scaled)
        
        return results
    
    def get_feature_importance(self, target: str = None) -> pd.DataFrame:
        """å–å¾—ç‰¹å¾µé‡è¦æ€§"""
        if target:
            return self.feature_importance.get(target)
        
        # åˆä½µæ‰€æœ‰ç›®æ¨™çš„ç‰¹å¾µé‡è¦æ€§
        all_importance = []
        for t, imp in self.feature_importance.items():
            imp_copy = imp.copy()
            imp_copy['target'] = t
            all_importance.append(imp_copy)
        
        return pd.concat(all_importance, ignore_index=True)
    
    def get_metrics_summary(self) -> pd.DataFrame:
        """å–å¾—æ¨¡å‹è©•ä¼°æ‘˜è¦"""
        return pd.DataFrame(self.metrics).T
    
    def save(self, filepath: str):
        """å„²å­˜æ¨¡å‹"""
        save_dict = {
            'models': self.models,
            'scaler_X': self.scaler_X,
            'config': self.config,
            'metrics': self.metrics,
            'feature_importance': self.feature_importance
        }
        with open(filepath, 'wb') as f:
            pickle.dump(save_dict, f)
        print(f"âœ… æ¨¡å‹å·²å„²å­˜è‡³: {filepath}")
    
    def load(self, filepath: str):
        """è¼‰å…¥æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            save_dict = pickle.load(f)
        
        self.models = save_dict['models']
        self.scaler_X = save_dict['scaler_X']
        self.config = save_dict['config']
        self.metrics = save_dict['metrics']
        self.feature_importance = save_dict['feature_importance']
        self._is_trained = True
        print(f"âœ… æ¨¡å‹å·²è¼‰å…¥: {filepath}")


def print_feature_importance_chart(predictor: CCLPredictor, target: str):
    """å°å‡ºç‰¹å¾µé‡è¦æ€§åœ–è¡¨ (ASCII)"""
    imp = predictor.get_feature_importance(target)
    if imp is None:
        print(f"No feature importance for {target}")
        return
    
    print(f"\nğŸ“Š {target} ç‰¹å¾µé‡è¦æ€§")
    print("-" * 50)
    
    max_imp = imp['importance'].max()
    for _, row in imp.iterrows():
        bar_len = int(row['importance'] / max_imp * 30)
        bar = 'â–ˆ' * bar_len
        print(f"{row['feature']:25s} {bar} {row['importance']:.3f}")


if __name__ == '__main__':
    # æ¸¬è©¦
    print("CCL æ¨¹è„‚é…æ–¹é æ¸¬æ¨¡å‹")
    print("=" * 50)
    
    # å»ºç«‹é æ¸¬å™¨
    predictor = CCLPredictor()
    
    # è¼‰å…¥æ•¸æ“š
    predictor.load_data('../data/ccl_resin_simulation.csv')
    
    # è¨“ç·´æ¨¡å‹
    scores = predictor.train(model_type='random_forest')
    
    # é¡¯ç¤ºè©•ä¼°çµæœ
    print("\nğŸ“ˆ æ¨¡å‹è©•ä¼°æ‘˜è¦:")
    print(predictor.get_metrics_summary())
    
    # é¡¯ç¤ºç‰¹å¾µé‡è¦æ€§
    for target in predictor.config.output_cols:
        print_feature_importance_chart(predictor, target)
    
    # æ¸¬è©¦é æ¸¬
    print("\nğŸ”® æ¸¬è©¦é æ¸¬:")
    test_formulation = {
        'Hardener_Eq_Ratio': 0.95,
        'Filler_Vol_Pct': 35,
        'FR_Wt_Pct': 5,
        'Toughener_Wt_Pct': 3,
        'Wash_Cycles': 4,
        'Residual_Cl_ppm': 15
    }
    print(f"è¼¸å…¥é…æ–¹: {test_formulation}")
    predictions = predictor.predict(test_formulation)
    print(f"é æ¸¬çµæœ: {predictions}")
    
    # å„²å­˜æ¨¡å‹
    predictor.save('ccl_predictor.pkl')
